* Large Language Model
Starting date: <2025-04-08 Tue>
Basically this project aims at helping me to understand how
*Large Language Model* (LLM) works. The objective of this
is to build one from scratch and by only using homemade tools.

[[file:./images/llm-adventure-cover.png]]

* Theory
** The aganda
1. Building an LLM
   1. Data preparation & sampling
   2. Attention mechanism
   3. LLM architecture
2. Foundation model
   1. Training loop
   2. Model evaluation
   3. Load pretrained weights
3. Finetuning
   1. Classification (Dataset with class labels)
   2. Personal assistant (Instruction dataset)

** Resources
- [Book] Build a Large Language Model (From Scratch) by /Sebastian Raschka/
- [Youtube] Tutorial of the book Build a Large Language Model: [[https://www.youtube.com/watch?v=kPGTx4wcm_w][serie]]
- [Youtube] tutorial serie: [[https://www.youtube.com/watch?v=ZLbVdvOoTKM][How to Build an LLM from Scratch]]
- [Youtube] tutorial: [[https://www.youtube.com/watch?v=kCc8FmEb1nY][Let's build GPT: from scratch, in code, spelled out.]]
- [Youtube] Short explaition in French of LLM: [[https://www.youtube.com/watch?v=LPZh9BOjkQs][Les modèles de langage en bref]]

** Data preparation & sampling
The model is to predict the next word in a sentence or a text. The model is
simply (pre)trained to predict the next word.

Next word **(/token)** prediction.

For example, with the given text: =This is a simple phrase to write=, the *Input* the LLM receives
is =This= (the first word of the sentence), the *target* (target to predict) is =is=, it is the second one.
And the remaing of the sentence =a simple phrase to write=, the LLM can't access words past the target.

The dataset is from the given example (=This is a simple phrase to write=) there are a sample N° for each
word, where the target move right until it reaches the end of the phrase. This how a dataset is prepare
for a /pre-training/.

This is ineficiant to put one phrase or text at a time,
in practice it prefered to do /batching/ to put multiple training input at a time. Each
input must have the same number of element. Ex:
Sample text:
=Hello world, this is a quiete longest text than the previous one, that were will
use for the pre-training dataset=.

Here the input are:
#+begin_src python
x = tensor(
    [["Hello", "world", ",", "this"],
    ["is", "a", "quiete", "longest"],
    ["text", "than", "the", "previous"],
    ...]
)
#+end_src

The common input lengths are > 1024.

The tokenization is the mechanism to assign a *Token ID* to each token of an input text.
To do so, we take an input text and to tokenize it, a seperator is chosen here is
the space (but it could be something else). And for each token of an input and ID
is assigned. There is what is called a *vocabulary* based on the unique *words* in
the training dataset.

There are more sophisticated process of play. Depending on the  models
uses different approach to tokenize.
For instance GPT use BPE (byte pair encoding), SentencePiece for Llama.
This more sophisticated approach aim to deal with unknown words in
order to not make the model crash if a word is unknown.

For example, this input text: =Akwirw ier=, unknown words are tokenized into individual
characters or subwords:
=Akwirw ier=
Tokens: =Ak=, =w=, =ir=, =w=, =" "=, =ier=
Token IDs: =33901=, =86=, =343=, =86=, =220=, =959=

With this if the words does not appear in the pre-training dataset, the model won't crash.

** LLM architecture
- Stuff to check RMS (root mean square: is a norm)
- SILU

#+CAPTION: GPT model architecture overview
#+NAME: fig:SED-GPT
[[file:./images/gpt_architecture.png]]

** PreTraining

*Labels* are the inputs shifted by +1. For example with this sample text:
=In the heart of the city stood the old library, a relic ...=

Tensor containing the inputs:
x = tensor([["In", "the", "heart", "of"],
            ["the", "city", "stood", the"],
	    ["old", "library", ",", "a"],
	    [..]])

Tensor containing the targets	    
x = tensor([["the", "heart", "of", "the"],
            ["city", "stood", the", "odl"],
	    ["library", ",", "a", "relic"],
	    [..]])
	    
* Hands on
** Building an LLM -- Stage 1
*** Data Preparation & Sampling
**** Tokenize Text

[[https://tiktokenizer.vercel.app/][Tiktokenizer Application]]: application to highlight the process of tokenization vwith the
various LLM model. A simple version of a tokenizer is available under [[dir:./sources/tokenizer]]

The strategy is straitforward split a text on a set of character (the one that match this
regular expression ="--|[.;_,?!'\"()]|\\s"=. Then it encodes each words by assigned
it a token ID which is an integer starting from 0 to N where N is the total unique
number of words in the pretraining dataset. This whole token IDs is called a *vocabulary*.
This method as limit for instance; it is not able to encode/decode words that are unknown,
this is why Byte Pair Encoding are used to!

**** Tokenizer v2 -- Byte Pair Encoding
The method is the same; however their is an extra layer of actions to do when an unknown
word is encounter. The words is split by chunk to identify known word letf to righ and going
smaller to small chunk size.

To develop this algorithm I used the following data structure
#+begin_src c
typedef struct {
    unsigned char *bytes;  // Pointer to byte array
    int length;            // Length of the byte array
} ByteSeq;
#+end_src

This represents a sequence of bytes - the fundamental unit in BPE. It stores both the raw byte data and its length.

#+begin_src c
typedef struct {
    ByteSeq *sequences;    // Array of byte sequences
    int count;             // Number of sequences
} ByteSequences;
#+end_src

This is a collection of ByteSeq objects, representing a tokenized piece of text where each token is a byte sequence.

#+begin_src c
typedef struct {
    unsigned char *first;       // First sequence in pair
    int first_len;              // Length of first sequence
    unsigned char *second;      // Second sequence in pair
    int second_len;             // Length of second sequence
    unsigned char *replacement; // New merged token
    int replacement_len;        // Length of replacement
    int count;                  // Frequency count of this pair
} Merge;
#+end_src

This structure represents a merge operation in BPE - it tracks:
- Two adjacent byte sequences to be merged
- The resulting merged sequence
- How frequently this pair appears in the data

#+begin_src c
typedef struct {
    unsigned char *bytes;  // Byte sequence for this vocabulary item
    int length;            // Length of the byte sequence
} VocabEntry;
#+end_src

An entry in the BPE vocabulary, storing a byte sequence that has been identified as a token.

In this implementation, the number of merge is fixed to the value **15**. 

**** Data Sampling with a Sliding Window
*** Attention Mechanism
